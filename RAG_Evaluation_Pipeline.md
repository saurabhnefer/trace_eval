# Evaluation Pipeline

## Overview
This document outlines the comprehensive evaluation pipeline for Retrieval-Augmented Generation (RAG) systems implemented as a modular package in `eval_main/rag_eval/`. The pipeline evaluates the performance of a coaching service by analyzing real user interactions stored in MongoDB.

## Package Structure
The evaluation pipeline is organized into the following modules:
- `config/`: Configuration settings and prompt templates
- `data/`: Data loading from MongoDB
- `retrieval/`: Document chunk retrieval
- `generation/`: Answer generation
- `evaluation/`: Answer evaluation using GPT-4o
- `utils/`: Utility functions
- `main.py`: Main entry point that orchestrates the pipeline

## Evaluation Dimensions
The evaluation covers multiple aspects of RAG system performance:
- Accuracy and faithfulness
- Relevance of retrieved chunks
- Coherence and clarity of responses
- Safety aspects (toxicity, jailbreak attempts)
- Business context adherence
- Factual accuracy (world knowledge)

## Pipeline Architecture
The evaluation pipeline follows these steps:
1. Check if the current day is Sunday (automatic weekly execution)
2. Load real user queries and responses from MongoDB for the past week
3. Process each conversation by:
   - Extracting the user query, retrieved chunks, and generated answer
   - Evaluating the answer using GPT-4o as a judge
   - Logging all metrics and results to Langfuse
   - Saving evaluation results to MongoDB in the `RAG_Evaluations` collection
4. Generate comprehensive evaluation scores and judgments

## Scheduled Execution
The pipeline is designed to run automatically every Sunday to provide weekly evaluation reports:
1. Verifies the current day is Sunday before proceeding
2. Collects and evaluates data from the previous 7 days
3. Can be forced to run on other days using the `--force-run` flag

## Evaluation Process
For each conversation, the pipeline:
1. Creates a Langfuse trace to track the entire evaluation
2. Logs the chunks retrieved for the question
3. Logs the answer generated by the coaching service
4. Evaluates the answer using a comprehensive rubric
5. Records all metrics and judgments in Langfuse with enhanced metadata including day of week
6. Saves the complete evaluation results to MongoDB for persistence and integration

## Evaluation Rubric

### I. Safety Evaluation
- **User-Query Safety**
  - Jailbreak Attempt (0-10)
  - Toxicity (0-10)
- **AI-Response Safety**
  - Jailbreak Success (0-10)
  - Toxicity (0-10)

### II. Accuracy & Faithfulness Evaluation
- **Factual Accuracy (World Knowledge)** (0-10)
- **Business Context Adherence** (0-10)
- **Holistic Contextual Accuracy & Faithfulness** (0-10)

### III. Answer-Chunk Relevance
- **Answer-Chunk Relevance** (0-10): Measures how well the answer is grounded in the retrieved chunks

### IV. Coherence & Clarity Evaluation
- **Coherence & Clarity** (0-10): Evaluates the logical flow and understandability of the response

## Configuration Options
The pipeline supports several command-line arguments:
- `--guest`: Use Guest_Message_History collection instead of Message_History
- `--limit`: Maximum number of conversations to evaluate (default: 20)
- `--no-date-filter`: Disable filtering by date range
- `--force-run`: Override the Sunday-only execution and run on any day

## Data Collection
By default, the pipeline:
- Runs only on Sundays
- Collects data from the previous 7 days
- Creates a comprehensive weekly evaluation report

## Implementation Details
- Uses OpenAI's GPT-4o model for evaluation
- Integrates with Langfuse for comprehensive tracking and analytics
- Connects to MongoDB to retrieve real user conversations
- Stores evaluation results in MongoDB for persistence and integration
- Generates detailed tags for each evaluation to enable filtering in Langfuse
- Enhanced tracing includes day of week and consistent timestamps

## Metrics and Reporting
All evaluation results are:
1. Logged to the console for real-time monitoring
2. Stored in Langfuse with comprehensive metadata
3. Saved to MongoDB in the `RAG_Evaluations` collection
4. Tagged for easy filtering and analysis
5. Organized by evaluation week for trend analysis

## MongoDB Integration
The evaluation results are saved to a dedicated MongoDB collection:
- **Collection Name**: `RAG_Evaluations`
- **Document Structure**:
  - `chat_id`: ID of the evaluated chat
  - `message_id`: ID of the specific message
  - `aiResponseMessageid`: ID of the AI response
  - `guest_mode`: Whether the chat is from guest mode
  - `evaluation_date`: Timestamp of the evaluation
  - `query`: The original user question
  - `chunks`: The retrieved document chunks used for context
  - `answer`: The generated answer
  - `evaluation_results`: Complete evaluation data
  - Top-level score fields for easy querying:
    - `accuracy_score`, `relevance_score`, `coherence_score`, etc.
    - `accuracy_judgment`, `relevance_judgment`, `coherence_judgment`, etc.

## Usage
Run the script with appropriate command-line arguments to evaluate the RAG system:
```
python eval_main/main.py [--guest] [--limit N] [--no-date-filter] [--force-run]
```

Results can be viewed and analyzed in the Langfuse dashboard and queried from the MongoDB `RAG_Evaluations` collection. For the standard weekly evaluation, no arguments are needed as the script will automatically run on Sundays and collect the past week's data. 